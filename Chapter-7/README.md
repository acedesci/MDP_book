# Chapter-7

This chapter provides an overview of finite state and action average reward MDP models. 
It focusses primarily on the unichain case in which the Markov chain corresponding to every determistic stationary policy has a single closed class.
Consequently optimal value functions can be found with a single optimality equation in contrast to
the multi-chain case which require a pair of nested optimality equations.
It focusses on the gain and bias of a policy and relates them to discounted and finite horizon value functions.

The chapter combines results from Chapters 8 and 9 in Puterman's 1994 book and is hopefully more accessible. 
Underlying Markov chain concepts appear in an Appendix which we will upload soon.
